{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9e75de04-a8e3-4082-9135-1e63b27dadb8",
   "metadata": {},
   "source": [
    "Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc4fca2-69a6-462a-a1e0-9a0a8b715dce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa76ec3e-8b1d-43c4-8e0a-1b12820f1d76",
   "metadata": {},
   "source": [
    "Set Visualization Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a54ec5-6c0d-443c-a826-904df6dc8afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "245bbfa5-f9ec-4866-b4a1-f2da6cad8568",
   "metadata": {},
   "source": [
    "Load and Validate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c2be2-90c6-4530-a893-ec5498bc83e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Load dataset with validation checks\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Dataset file not found: {file_path}\")\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(f\"Warning: Dataset contains missing values:\\n{missing_values[missing_values > 0]}\")\n",
    "    \n",
    "    print(f\"Dataset loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    return df\n",
    "\n",
    "# File path - update with your actual path\n",
    "file_path = \"Logistic_Regression_E-Commerce_Behavior_Dataset.csv\"\n",
    "try:\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # Display basic information about the dataset\n",
    "    print(\"\\nDataset Summary:\")\n",
    "    print(df.describe().round(2))\n",
    "    \n",
    "    # Display correlation heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    # If file doesn't exist, create sample data for demonstration\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Creating sample dataset for demonstration purposes...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    time_on_site = np.random.normal(5, 2, n_samples)\n",
    "    pages_viewed = np.random.poisson(3, n_samples)\n",
    "    referred_by_ad = np.random.binomial(1, 0.3, n_samples)\n",
    "    previous_purchases = np.random.poisson(1.5, n_samples)\n",
    "    location_score = np.random.uniform(0, 10, n_samples)\n",
    "    \n",
    "    # Create purchase probability based on features\n",
    "    logits = (0.5 * time_on_site + \n",
    "              1.2 * pages_viewed + \n",
    "              0.8 * referred_by_ad + \n",
    "              0.6 * previous_purchases + \n",
    "              0.3 * location_score - 5)\n",
    "    \n",
    "    prob = 1 / (1 + np.exp(-logits))\n",
    "    purchase_made = (np.random.random(n_samples) < prob).astype(int)\n",
    "    \n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'Time_on_Site': time_on_site,\n",
    "        'Pages_Viewed': pages_viewed,\n",
    "        'Referred_by_Ad': referred_by_ad,\n",
    "        'Previous_Purchases': previous_purchases,\n",
    "        'Location_Score': location_score,\n",
    "        'Purchase_Made': purchase_made\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0a9e0df-71f6-4edd-b506-1df61ed68ef4",
   "metadata": {},
   "source": [
    "Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d4f85c-bddc-47bf-9467-161f06565140",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing features and target variables...\")\n",
    "# Check if these columns exist in the dataframe\n",
    "expected_columns = ['Time_on_Site', 'Pages_Viewed', 'Referred_by_Ad', 'Previous_Purchases', 'Location_Score', 'Purchase_Made']\n",
    "missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"Warning: The following expected columns are missing from the dataset: {missing_columns}\")\n",
    "    print(\"Available columns in the dataset:\", df.columns.tolist())\n",
    "    # If columns are missing, we'll use whatever columns are available\n",
    "    # excluding the target variable\n",
    "    feature_columns = [col for col in df.columns if col != 'Purchase_Made' and col in df.columns]\n",
    "    if 'Purchase_Made' not in df.columns:\n",
    "        raise ValueError(\"Error: 'Purchase_Made' column is missing from the dataset. Cannot proceed without target variable.\")\n",
    "else:\n",
    "    feature_columns = ['Time_on_Site', 'Pages_Viewed', 'Referred_by_Ad', 'Previous_Purchases', 'Location_Score']\n",
    "\n",
    "# Print the features we're using\n",
    "print(f\"Using these features: {feature_columns}\")\n",
    "\n",
    "# Create feature matrix and target vector\n",
    "X = df[feature_columns]\n",
    "y = df['Purchase_Made']\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "\n",
    "# Create train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fecc20e5-f324-4943-af9d-87ed6681d30f",
   "metadata": {},
   "source": [
    "Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c680ba-d6f8-40a0-b426-8b1455238a4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nTraining logistic regression model...\")\n",
    "try:\n",
    "    # Verify data exists and has the right format\n",
    "    print(f\"X_train type: {type(X_train)}, shape: {X_train.shape}\")\n",
    "    print(f\"y_train type: {type(y_train)}, shape: {y_train.shape}\")\n",
    "    \n",
    "    # Check for NaN values\n",
    "    if X_train.isna().any().any():\n",
    "        print(\"Warning: X_train contains NaN values. Imputing with mean values...\")\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "        X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    # Train model with more diagnostic info\n",
    "    model = LogisticRegression(max_iter=1000, solver='liblinear')  # liblinear works better for smaller datasets\n",
    "    print(\"Fitting model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Model training complete!\")\n",
    "    \n",
    "    # Check if model was fit successfully\n",
    "    print(f\"Model coefficient shape: {model.coef_.shape}\")\n",
    "    print(f\"Model intercept: {model.intercept_}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error training model: {str(e)}\")\n",
    "    print(\"Debug information:\")\n",
    "    print(f\"- Are X_train and y_train defined? X_train: {'Defined' if 'X_train' in locals() else 'Not defined'}, y_train: {'Defined' if 'y_train' in locals() else 'Not defined'}\")\n",
    "    \n",
    "    # Try to recover if possible by creating synthetic data\n",
    "    if 'X_train' not in locals() or 'y_train' not in locals():\n",
    "        print(\"Creating synthetic data for demonstration...\")\n",
    "        np.random.seed(42)\n",
    "        n_samples = 1000\n",
    "        n_features = 5\n",
    "        \n",
    "        X_train = np.random.randn(int(n_samples*0.7), n_features)\n",
    "        y_train = np.random.randint(0, 2, size=int(n_samples*0.7))\n",
    "        X_test = np.random.randn(int(n_samples*0.3), n_features)\n",
    "        y_test = np.random.randint(0, 2, size=int(n_samples*0.3))\n",
    "        \n",
    "        feature_names = ['Feature_' + str(i+1) for i in range(n_features)]\n",
    "        X_train = pd.DataFrame(X_train, columns=feature_names)\n",
    "        X_test = pd.DataFrame(X_test, columns=feature_names)\n",
    "        \n",
    "        print(\"Created synthetic data with features:\", feature_names)\n",
    "        \n",
    "        # Train with synthetic data\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"Model trained with synthetic data.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "610c2118-1975-4688-ba0f-4e31fce14fad",
   "metadata": {},
   "source": [
    "Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf97804-fd25-4050-a232-5788a11dc969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate model and display performance metrics\"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Display classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Display confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['No Purchase', 'Purchase'],\n",
    "                yticklabels=['No Purchase', 'Purchase'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display model coefficients\n",
    "    coefficients = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Coefficient': model.coef_[0],\n",
    "        'Abs_Magnitude': np.abs(model.coef_[0])\n",
    "    })\n",
    "    \n",
    "    coef_sorted = coefficients.sort_values(by='Abs_Magnitude', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Coefficient', y='Feature', data=coef_sorted, palette='viridis')\n",
    "    plt.title('Feature Importance (Logistic Regression Coefficients)')\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.grid(axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_prob\n",
    "\n",
    "# Evaluate full model\n",
    "y_prob = evaluate_model(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34c1d4eb-8330-4e74-b300-5a49ea321da1",
   "metadata": {},
   "source": [
    "Sigmoid Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee44f12-d28b-4771-b6d9-87c63356f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "# Create a single static visualization with threshold=0.5\n",
    "print(\"Creating static visualization with threshold=0.5...\")\n",
    "\n",
    "# Fit logistic regression using only the single feature\n",
    "X_single_feature = df['Pages_Viewed']\n",
    "feature_name = 'Pages Viewed'\n",
    "y = df['Purchase_Made']\n",
    "threshold = 0.5\n",
    "\n",
    "X_single = X_single_feature.values.reshape(-1, 1)\n",
    "model_single = LogisticRegression(max_iter=1000)\n",
    "model_single.fit(X_single, y)\n",
    "\n",
    "# Display model info\n",
    "print(f\"Single feature model coefficients: {model_single.coef_}\")\n",
    "print(f\"Single feature model intercept: {model_single.intercept_}\")\n",
    "\n",
    "# Set up plot\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Generate prediction range\n",
    "x_min, x_max = X_single.min() - 0.5, X_single.max() + 0.5\n",
    "x_range = np.linspace(x_min, x_max, 300).reshape(-1, 1)\n",
    "probs = model_single.predict_proba(x_range)[:, 1]\n",
    "\n",
    "# Calculate theoretical sigmoid range needed to go from 0.05 to 0.95 probability\n",
    "logit_05 = np.log(0.05/(1-0.05))\n",
    "logit_95 = np.log(0.95/(1-0.95))\n",
    "\n",
    "x_05 = (logit_05 - model_single.intercept_[0]) / model_single.coef_[0][0]\n",
    "x_95 = (logit_95 - model_single.intercept_[0]) / model_single.coef_[0][0]\n",
    "\n",
    "# Ensure plot range covers the important decision boundaries\n",
    "x_min = min(x_min, x_05 - 1)\n",
    "x_max = max(x_max, x_95 + 1)\n",
    "\n",
    "# Update x_range with new bounds\n",
    "x_range = np.linspace(x_min, x_max, 300).reshape(-1, 1)\n",
    "probs = model_single.predict_proba(x_range)[:, 1]\n",
    "\n",
    "# Function to predict with custom threshold\n",
    "def predict_with_threshold(X, threshold):\n",
    "    return (model_single.predict_proba(X)[:, 1] >= threshold).astype(int)\n",
    "\n",
    "# Get predictions using the threshold\n",
    "y_pred = predict_with_threshold(X_single, threshold)\n",
    "misclassified_mask = y_pred != y.values\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "# Plot elements\n",
    "# Points colored by actual outcome\n",
    "points_no = ax.scatter(X_single[y==0], np.zeros(sum(y==0)), \n",
    "                     alpha=0.4, color='blue', s=50, label='No Purchase (Actual)')\n",
    "points_yes = ax.scatter(X_single[y==1], np.ones(sum(y==1)), \n",
    "                      alpha=0.4, color='red', s=50, label='Purchase (Actual)')\n",
    "\n",
    "# Sigmoid curve\n",
    "line, = ax.plot(x_range, probs, color='purple', linewidth=3, \n",
    "               label='Sigmoid Curve (Probability)')\n",
    "\n",
    "# Threshold line and text\n",
    "threshold_line = ax.axhline(threshold, color='black', linestyle='--', linewidth=2)\n",
    "threshold_text = ax.text(0.95, 0.05, f'Threshold = {threshold:.2f}', transform=ax.transAxes, \n",
    "                       ha='right', fontsize=12, \n",
    "                       bbox=dict(facecolor='white', alpha=0.7, boxstyle='round'))\n",
    "\n",
    "# Decision boundary line and annotation\n",
    "boundary_x = (np.log(threshold/(1-threshold)) - model_single.intercept_[0]) / model_single.coef_[0][0]\n",
    "\n",
    "# Check if boundary is within plot range\n",
    "if boundary_x >= x_min and boundary_x <= x_max:\n",
    "    boundary_line = ax.axvline(boundary_x, color='green', linestyle='-', linewidth=2,\n",
    "                             label='Decision Boundary')\n",
    "    \n",
    "    # Add annotation to make boundary more noticeable\n",
    "    ax.annotate(f\"Decision Boundary\\n({boundary_x:.2f})\", \n",
    "               xy=(boundary_x, 0.5), xytext=(boundary_x+0.5, 0.5),\n",
    "               arrowprops=dict(facecolor='green', shrink=0.05),\n",
    "               bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"green\", alpha=0.8))\n",
    "else:\n",
    "    # Add text explaining out-of-range boundary\n",
    "    if boundary_x > x_max:\n",
    "        ax.text(0.98, 0.2, f\"Decision Boundary\\nat x={boundary_x:.2f}\\n(outside view)\", \n",
    "               transform=ax.transAxes, ha='right',\n",
    "               bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"green\", alpha=0.8))\n",
    "    else:\n",
    "        ax.text(0.02, 0.2, f\"Decision Boundary\\nat x={boundary_x:.2f}\\n(outside view)\", \n",
    "               transform=ax.transAxes, ha='left',\n",
    "               bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"green\", alpha=0.8))\n",
    "\n",
    "# Accuracy text\n",
    "accuracy_text = ax.text(0.05, 0.95, f'Accuracy: {accuracy:.2f}', transform=ax.transAxes, \n",
    "                      ha='left', fontsize=12, \n",
    "                      bbox=dict(facecolor='white', alpha=0.7, boxstyle='round'))\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel(f\"{feature_name}\", fontsize=12)\n",
    "ax.set_ylabel(\"Probability of Purchase\", fontsize=12)\n",
    "ax.set_title(f\"Sigmoid Curve with Decision Threshold at {threshold}\\nFeature: {feature_name}\", \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.2)  # Make room for legend\n",
    "\n",
    "# Save the figure\n",
    "save_path = 'sigmoid_static.png'\n",
    "try:\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Visualization saved to {save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving visualization: {e}\")\n",
    "\n",
    "# Show the single plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63c81a94-b56c-487c-a74d-3245b592218d",
   "metadata": {},
   "source": [
    "Explaining the Sigmoid Curve and Logistic Regression\n",
    "This image provides an excellent visualization of how logistic regression works. Here's a simple explanation you can share with your class:\n",
    "The Basics\n",
    "Logistic regression helps us predict binary outcomes - in this case, whether a customer will make a purchase (1) or not (0) based on how many pages they viewed on a website.\n",
    "Key Elements in the Image\n",
    "\n",
    "The Purple Curve (Sigmoid Curve): This S-shaped curve shows the probability of a purchase happening based on the number of pages viewed. Notice how it smoothly transitions from close to 0% probability (bottom left) to nearly 100% probability (top right).\n",
    "The Data Points:\n",
    "\n",
    "Blue dots at the bottom (y=0) represent customers who did NOT make a purchase\n",
    "Red dots at the top (y=1) represent customers who DID make a purchase\n",
    "\n",
    "\n",
    "The Black Dashed Line: This horizontal line at 0.5 represents our decision threshold. Any probability above this line is classified as \"Will Purchase\" and below as \"Won't Purchase.\"\n",
    "The Green Vertical Line: This is the decision boundary at 6.37 pages. It shows the exact point where our prediction flips from \"Won't Purchase\" to \"Will Purchase.\"\n",
    "The Accuracy: The model achieves 75% accuracy with this threshold, meaning it correctly classifies 75% of all customers.\n",
    "\n",
    "Intuitive Explanation\n",
    "Think of it like this: When a customer browses very few pages (left side), they're unlikely to purchase. When they view many pages (right side), they're much more likely to purchase.\n",
    "The magic happens at 6.37 pages - this is our \"tipping point.\" If someone views fewer than 6.37 pages, we predict they won't buy. If they view more, we predict they will buy.\n",
    "The sigmoid shape is important because it captures how probability doesn't change linearly with page views. The first few pages don't increase purchase probability much, but there's a \"sweet spot\" in the middle (around 4-8 pages) where each additional page viewed significantly increases purchase likelihood. Eventually, the curve flattens as additional pages have diminishing returns.\n",
    "Why This Matters\n",
    "This visualization demonstrates how a simple model can make reasonable predictions about customer behavior using just one feature. By setting the right threshold (currently 0.5), businesses can balance between false positives and false negatives based on their specific needs."
   ]
  },
  {
   "cell_type": "raw",
   "id": "21c92781-d179-4330-a5d7-10b27566656e",
   "metadata": {},
   "source": [
    "Threshold Effect on Multiple Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c4601-619d-4b14-aae3-13e59c5e3939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_threshold_effects(model, X_test, y_test):\n",
    "    \"\"\"Plot accuracy, precision, recall as threshold changes\"\"\"\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    \n",
    "    # Get probabilities\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Test different thresholds\n",
    "    thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        precision.append(precision_score(y_test, y_pred, zero_division=0))\n",
    "        recall.append(recall_score(y_test, y_pred, zero_division=0))\n",
    "        f1.append(f1_score(y_test, y_pred, zero_division=0))\n",
    "    \n",
    "    # Plot metrics vs threshold\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(thresholds, accuracy, 'o-', label='Accuracy', linewidth=2)\n",
    "    plt.plot(thresholds, precision, 's-', label='Precision', linewidth=2)\n",
    "    plt.plot(thresholds, recall, '^-', label='Recall', linewidth=2)\n",
    "    plt.plot(thresholds, f1, 'D-', label='F1 Score', linewidth=2)\n",
    "    \n",
    "    # Add vertical line at default threshold 0.5\n",
    "    plt.axvline(x=0.5, color='black', linestyle='--', alpha=0.7, label='Default Threshold (0.5)')\n",
    "    \n",
    "    # Find and mark optimal F1 threshold\n",
    "    optimal_idx = np.argmax(f1)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    plt.scatter([optimal_threshold], [f1[optimal_idx]], s=100, c='red', \n",
    "               marker='*', label=f'Optimal F1 Threshold ({optimal_threshold:.2f})')\n",
    "    \n",
    "    plt.xlabel('Threshold Value', fontsize=12)\n",
    "    plt.ylabel('Metric Score', fontsize=12)\n",
    "    plt.title('Effect of Threshold on Classification Metrics', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot threshold effects for full model\n",
    "plot_threshold_effects(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70f8b277-2d58-41b0-97b5-98f4fa29df4a",
   "metadata": {},
   "source": [
    "3D Visualization for Two Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85abd232-0519-4156-8bd7-7a934ea0f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_sigmoid(df, feature1='Pages_Viewed', feature2='Time_on_Site'):\n",
    "    \"\"\"Create 3D visualization of sigmoid surface with two features\"\"\"\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    \n",
    "    # Fit model with two features\n",
    "    X_3d = df[[feature1, feature2]]\n",
    "    y_3d = df['Purchase_Made']\n",
    "    \n",
    "    model_3d = LogisticRegression(max_iter=1000)\n",
    "    model_3d.fit(X_3d, y_3d)\n",
    "    \n",
    "    # Create meshgrid for surface\n",
    "    x1_min, x1_max = X_3d[feature1].min() - 1, X_3d[feature1].max() + 1\n",
    "    x2_min, x2_max = X_3d[feature2].min() - 1, X_3d[feature2].max() + 1\n",
    "    \n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100),\n",
    "                         np.linspace(x2_min, x2_max, 100))\n",
    "    \n",
    "    # Get predicted probabilities for all mesh points\n",
    "    grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    Z = model_3d.predict_proba(grid)[:, 1]\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    \n",
    "    # Create 3D plot\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot surface\n",
    "    surf = ax.plot_surface(xx1, xx2, Z, cmap='viridis', alpha=0.7, antialiased=True)\n",
    "    \n",
    "    # Plot threshold plane at z=0.5\n",
    "    ax.plot_surface(xx1, xx2, np.ones(Z.shape)*0.5, color='red', alpha=0.2)\n",
    "    \n",
    "    # Plot decision boundary contour\n",
    "    ax.contour(xx1, xx2, Z, levels=[0.5], colors='red', linestyles='solid', linewidths=2)\n",
    "    \n",
    "    # Plot training points\n",
    "    ax.scatter(X_3d[feature1][y_3d==0], X_3d[feature2][y_3d==0], np.zeros(sum(y_3d==0)), \n",
    "             c='blue', marker='o', label='No Purchase', alpha=0.5)\n",
    "    ax.scatter(X_3d[feature1][y_3d==1], X_3d[feature2][y_3d==1], np.ones(sum(y_3d==1)), \n",
    "             c='red', marker='^', label='Purchase', alpha=0.5)\n",
    "    \n",
    "    # Labels and formatting\n",
    "    ax.set_xlabel(feature1, fontsize=12)\n",
    "    ax.set_ylabel(feature2, fontsize=12)\n",
    "    ax.set_zlabel('Probability of Purchase', fontsize=12)\n",
    "    ax.set_title('3D Sigmoid Surface with Decision Boundary', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add colorbar\n",
    "    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5, label='Probability')\n",
    "    \n",
    "    # Set viewing angle\n",
    "    ax.view_init(elev=25, azim=45)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create 3D visualization\n",
    "plot_3d_sigmoid(df)\n",
    "\n",
    "print(\"\\nAnalysis complete! All visualizations have been displayed and animations created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb85c60-63bf-4756-ba95-ac3e80c30d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aba0d7e-9ae0-4a13-9559-8ceb295e90be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7391b-16f3-46f0-bf38-1c6f44ae21af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
